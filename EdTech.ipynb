{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sklearn.metrics import roc_curve, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train = pd.read_csv('EdTech_train_data.csv')\n",
    "\n",
    "X = train.drop(['No','Rank'],axis=1).values\n",
    "y = train['Rank'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Education</th>\n",
       "      <th>Major</th>\n",
       "      <th>Year</th>\n",
       "      <th>TargetMajor</th>\n",
       "      <th>Degree Application</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GMAT</th>\n",
       "      <th>IBT</th>\n",
       "      <th>GRE</th>\n",
       "      <th>IETLS</th>\n",
       "      <th>Distinguished Internship</th>\n",
       "      <th>General Internship</th>\n",
       "      <th>Research Experience</th>\n",
       "      <th>Other Experience</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>237</td>\n",
       "      <td>2010</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>327.500000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>271</td>\n",
       "      <td>3</td>\n",
       "      <td>159</td>\n",
       "      <td>2009</td>\n",
       "      <td>349</td>\n",
       "      <td>1</td>\n",
       "      <td>3.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>272.211083</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>790</td>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>2011</td>\n",
       "      <td>293</td>\n",
       "      <td>3</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>325.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>579</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>2008</td>\n",
       "      <td>372</td>\n",
       "      <td>1</td>\n",
       "      <td>3.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>479</td>\n",
       "      <td>3</td>\n",
       "      <td>224</td>\n",
       "      <td>2009</td>\n",
       "      <td>275</td>\n",
       "      <td>2</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    No  Education  Major  Year  TargetMajor  Degree Application   GPA  GMAT  \\\n",
       "0  187          3    237  2010          171                   1  3.70   0.0   \n",
       "1  271          3    159  2009          349                   1  3.38   0.0   \n",
       "2  790          3    273  2011          293                   3  3.40   0.0   \n",
       "3  579          3    299  2008          372                   1  3.80   0.0   \n",
       "4  479          3    224  2009          275                   2  3.27   0.0   \n",
       "\n",
       "     IBT         GRE  IETLS  Distinguished Internship  General Internship  \\\n",
       "0  102.0  327.500000    7.0                         4                   2   \n",
       "1   97.0  272.211083    5.5                         2                   2   \n",
       "2  101.0  325.000000    7.0                         4                   2   \n",
       "3    0.0    0.000000    0.0                         1                   0   \n",
       "4   88.0  320.000000    6.0                         2                   0   \n",
       "\n",
       "   Research Experience  Other Experience  Rank  \n",
       "0                    6                 0     3  \n",
       "1                    5                 0     2  \n",
       "2                    6                 5     3  \n",
       "3                    1                 0     0  \n",
       "4                    5                 0     2  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.eval()\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# standardize the data attributes\n",
    "standardized_X = preprocessing.scale(X)\n",
    "# normalize the data attributes\n",
    "normalized_X = preprocessing.normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8681318681318682\n",
      "accuracy: 0.8131868131868132\n",
      "accuracy: 0.8571428571428571\n",
      "accuracy: 0.8222222222222222\n",
      "accuracy: 0.9\n",
      "accuracy: 0.9222222222222223\n",
      "accuracy: 0.8777777777777778\n",
      "accuracy: 0.8666666666666667\n",
      "accuracy: 0.8539325842696629\n",
      "accuracy: 0.8863636363636364\n",
      "mean_acc 0.8667646647983727\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "accs = []\n",
    "for train_index, test_index in skf.split(standardized_X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    accs.append(acc)\n",
    "    print('accuracy:', acc)\n",
    "print('mean_acc',np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model,14)\n",
    "rfe = rfe.fit(X, y)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.46      0.55       160\n",
      "          1       0.71      0.77      0.74       239\n",
      "          2       0.89      0.98      0.93       333\n",
      "          3       0.97      1.00      0.99       168\n",
      "\n",
      "avg / total       0.82      0.83      0.82       900\n",
      "\n",
      "[[ 73  72  15   0]\n",
      " [ 29 183  27   0]\n",
      " [  2   1 325   5]\n",
      " [  0   0   0 168]]\n",
      "[0.80662983 0.83425414 0.81767956 0.81005587 0.83146067]\n",
      "Accuracy: 0.82 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# use cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.29      0.42       160\n",
      "          1       0.66      0.90      0.76       239\n",
      "          2       0.95      0.98      0.97       333\n",
      "          3       1.00      0.98      0.99       168\n",
      "\n",
      "avg / total       0.85      0.84      0.82       900\n",
      "\n",
      "[0.83977901 0.77348066 0.85082873 0.83240223 0.81460674]\n",
      "Accuracy: 0.82 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "# print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# use cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.59      0.57       160\n",
      "          1       0.53      0.58      0.56       239\n",
      "          2       0.60      0.68      0.64       333\n",
      "          3       0.72      0.42      0.53       168\n",
      "\n",
      "avg / total       0.60      0.59      0.58       900\n",
      "\n",
      "[[ 94  33  27   6]\n",
      " [ 32 139  58  10]\n",
      " [ 32  65 225  11]\n",
      " [ 10  23  65  70]]\n",
      "[0.46961326 0.38121547 0.35911602 0.33519553 0.43258427]\n",
      "Accuracy: 0.40 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# fit a k-nearest neighbor model to the data\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "# use cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85635359 0.86740331 0.83977901 0.82681564 0.86516854]\n",
      "Accuracy: 0.85 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# fit a CART model to the data\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "#model.fit(X, y)\n",
    "'''print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "'''\n",
    "\n",
    "# use cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1592495.48292073 -1830665.90843967 -1800987.38371087 -2126852.0348177\n",
      " -1790606.6070179  -1866615.9778919  -1348854.93343377 -1898973.65282763\n",
      " -1842601.15305884 -1679324.47637093]\n",
      "Accuracy: -1777797.76 (+/- 389940.19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "# fit a SVM model to the data\n",
    "model = KMeans()\n",
    "'''model.fit(X, y)\n",
    "print(model)\n",
    "# make predictions\n",
    "expected = y\n",
    "predicted = model.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "'''\n",
    "# use cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=10)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid={'alpha': array([1.e+00, 1.e-01, 1.e-02, 1.e-03, 1.e-04, 0.e+00])},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0)\n",
      "0.8288717651309263\n",
      "1.0\n",
      "[0.80370133 0.80374185 0.82941926 0.84673366 0.86400241]\n",
      "Accuracy: 0.83 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# prepare a range of alpha values to test\n",
    "alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "# create and fit a ridge regression model, testing each alpha\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(estimator=model, param_grid=dict(alpha=alphas))\n",
    "grid.fit(X, y)\n",
    "print(grid)\n",
    "# summarize the results of the grid search\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)\n",
    "\n",
    "# use cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(grid, X, y, cv=5)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV(cv=None, error_score='raise',\n",
      "          estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
      "          fit_params=None, iid=True, n_iter=100, n_jobs=1,\n",
      "          param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000210BA5BA6A0>},\n",
      "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
      "          return_train_score='warn', scoring=None, verbose=0)\n",
      "0.8288717399094397\n",
      "0.9956005524208634\n",
      "[0.80370179 0.80374076 0.82941926 0.84673368 0.86400232]\n",
      "Accuracy: 0.83 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform as sp_rand\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# prepare a uniform distribution to sample for the alpha parameter\n",
    "param_grid = {'alpha': sp_rand()}\n",
    "# create and fit a ridge regression model, testing random alpha values\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100)\n",
    "rsearch.fit(X, y)\n",
    "print(rsearch)\n",
    "# summarize the results of the random parameter search\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(rsearch, X, y, cv=5)\n",
    "print (scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8466717831144045\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "#odel.fit(X,y)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv = 5)\n",
    "print (np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8021978021978022\n",
      "accuracy: 0.8681318681318682\n",
      "accuracy: 0.8241758241758241\n",
      "accuracy: 0.8555555555555555\n",
      "accuracy: 0.8777777777777778\n",
      "accuracy: 0.8444444444444444\n",
      "accuracy: 0.8\n",
      "accuracy: 0.8333333333333334\n",
      "accuracy: 0.8089887640449438\n",
      "accuracy: 0.875\n",
      "mean_acc 0.8389605369661549\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "accs = []\n",
    "model = DecisionTreeClassifier()\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    accs.append(acc)\n",
    "    print('accuracy:', acc)\n",
    "print('mean_acc',np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9120879120879121\n",
      "accuracy: 0.9120879120879121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8461538461538461\n",
      "accuracy: 0.8555555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9\n",
      "accuracy: 0.9111111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8333333333333334\n",
      "accuracy: 0.8111111111111111\n",
      "accuracy: 0.898876404494382\n",
      "accuracy: 0.8977272727272727\n",
      "mean_acc 0.8778044458662437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\44994\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9120879120879121,\n",
       " 0.9120879120879121,\n",
       " 0.8461538461538461,\n",
       " 0.8555555555555555,\n",
       " 0.9,\n",
       " 0.9111111111111111,\n",
       " 0.8333333333333334,\n",
       " 0.8111111111111111,\n",
       " 0.898876404494382,\n",
       " 0.8977272727272727]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "accs = []\n",
    "model = xgb.XGBClassifier(subsample=0.5,nthread=7,objective='muti:softmax',num_class=1)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    accs.append(acc)\n",
    "    print('accuracy:', acc)\n",
    "print('mean_acc',np.mean(np.array(accs)))\n",
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model.predict(X_test)\n",
    "# predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# scores = cross_val_score(model, X, y, cv=5)\n",
    "# print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9120879120879121\n",
      "accuracy: 0.8791208791208791\n",
      "accuracy: 0.8681318681318682\n",
      "accuracy: 0.8888888888888888\n",
      "accuracy: 0.9111111111111111\n",
      "accuracy: 0.9111111111111111\n",
      "accuracy: 0.8555555555555555\n",
      "accuracy: 0.8666666666666667\n",
      "accuracy: 0.9101123595505618\n",
      "accuracy: 0.9090909090909091\n",
      "mean_acc 0.8911877261315462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "skf = StratifiedKFold(n_splits=10,shuffle=True,random_state=0)\n",
    "accs = []\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=2, max_depth=100)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test,y_pred)\n",
    "    accs.append(acc)\n",
    "    print('accuracy:', acc)\n",
    "print('mean_acc',np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
